name: "exp002_transunet"
description: "TransUNet training - CNN encoder + Transformer bottleneck + CNN decoder"
tags: ["transunet", "transformer", "hybrid"]

dataset: "configs/datasets/fives512_rgb.yaml"

output:
  dir: "outputs/experiments/exp002_transunet"
  save_predictions: true

data:
  batch_size: 1
  num_workers: 2
  pin_memory: true
  
  augmentation:
    enabled: false
  
  preprocessing:
    normalize: true
    pad_to_multiple: 32

model:
  type: "TransUNet"
  in_channels: 3
  out_channels: 1
  depths: [32, 64, 128, 256, 512]     # Same as UNet: [enc1, enc2, enc3, enc4, bottleneck]
  
  # Transformer configuration (reduced for memory efficiency)
  transformer_depth: 2                # Number of transformer blocks (reduced from 6)
  transformer_heads: 4                # Number of attention heads (reduced from 8)
  transformer_mlp_ratio: 2.0          # MLP expansion ratio (reduced from 4.0)
  transformer_dropout: 0.1            # Dropout probability
  
  final_activation: "sigmoid"

training:
  epochs: 30
  
  optimizer:
    type: "adam"
    learning_rate: 0.0001
    weight_decay: 0.0001
  
  scheduler:
    type: "cosine"
    min_lr: 0.000001
  
  loss:
    type: "dice"
    smooth: 0.000001
  
  metrics:
    - "dice"
    - "iou"
  
  mixed_precision: false
  gradient_clip: 1.0
  
  validation:
    frequency: 1
  
  early_stopping:
    enabled: true
    patience: 10
    monitor: "val_dice"
    mode: "max"
  
  checkpoint:
    save_best: true
    save_last: true
    monitor: "val_dice"
    mode: "max"

logging:
  tensorboard: true
  log_images: true
  image_log_frequency: 1              # Log images every N epochs (if log_images: true)
  
  # Advanced logging options
  log_activations: true              # Enable to monitor layer activations
  activation_log_frequency: 1         # Log activations every N epochs (if log_activations: true)
  activation_layers: "auto"           # "auto" for model defaults, list for custom, null for all

# Debug options for memory profiling
debug:
  profile_memory: true              # Enable memory profiling before training
  detailed_memory: true              # Show per-layer breakdown
  estimate_activations: true         # Estimate activation memory (runs forward pass)
  profile_training_step: true       # Profile an actual training step (CUDA only)

seed: 42
device: "cuda" 
