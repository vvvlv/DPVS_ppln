name: "exp003_utrans"
description: "UTrans: UNet with Transformer bottleneck for global context"
tags: ["utrans", "transformer", "hybrid", "baseline"]

dataset: "configs/datasets/fives_512.yaml"

data:
  batch_size: 2  # Smaller batch due to transformer memory requirements
  num_workers: 2
  pin_memory: true
  
  augmentation:
    enabled: false
  
  preprocessing:
    normalize: true
    pad_to_multiple: 32

model:
  type: "UTrans"
  in_channels: 3
  out_channels: 1
  depths: [64, 128, 256, 512, 1024]  # Standard UNet channel progression
  
  # Transformer configuration
  transformer_depth: 4              # Number of transformer blocks in bottleneck
  transformer_heads: 8              # Number of attention heads
  transformer_mlp_ratio: 4.0        # FFN hidden dim = 4 * embed_dim
  transformer_dropout: 0.1          # Dropout probability
  
  final_activation: "sigmoid"

training:
  epochs: 20
  
  optimizer:
    type: "adam"
    learning_rate: 0.0001           # Same as other models
    weight_decay: 0.0001
  
  scheduler:
    type: "cosine"
    min_lr: 0.000001
  
  loss:
    type: "dice"
    smooth: 0.000001
  
  metrics:
    - "dice"
    - "iou"
  
  mixed_precision: false
  gradient_clip: 1.0
  
  validation:
    frequency: 1
  
  early_stopping:
    enabled: true
    patience: 5
    monitor: "val_dice"
    mode: "max"
  
  checkpoint:
    save_best: true
    save_last: true
    monitor: "val_dice"
    mode: "max"

logging:
  tensorboard: true
  log_images: false

output:
  dir: "outputs/experiments/exp003_utrans"
  save_predictions: true

seed: 42
device: "cuda"

