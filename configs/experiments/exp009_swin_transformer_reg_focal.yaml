name: "exp009_swin_transformer_reg_focal"
description: "SwinTransformerSys (full Swin-UNet) with regularization, Focal Tversky loss"
tags: ["exp009", "swin_transformer", "transformer", "focal_tversky", "regularized", "g512"]

dataset: "configs/datasets/fives512_g.yaml"

output:
  dir: "outputs/experiments/exp009_swin_transformer_reg_focal"
  save_predictions: true

data:
  batch_size: 2
  num_workers: 4
  pin_memory: true
  
  augmentation:
    enabled: true
    horizontal_flip: 0.5
    vertical_flip: 0.2
    rotation90: true
    rotation_prob: 0.5
    brightness: 0.1
  
  preprocessing:
    normalize: true
    pad_to_multiple: 32

model:
  type: "SwinTransformerSys"
  in_channels: 1
  out_channels: 1
  image_size: [512, 512]
  patch_size: 4
  embed_dim: 96                    # Base embedding dimension
  depths: [2, 2, 6, 2]            # Encoder depths per stage
  depths_decoder: [2, 2, 2, 2]    # Decoder depths per stage (can differ from encoder)
  num_heads: [3, 6, 12, 24]        # Attention heads per stage
  window_size: 8                    # Window size for attention
  mlp_ratio: 4.0
  qkv_bias: true
  qk_scale: null
  drop_rate: 0.1                    # Dropout rate
  attn_drop_rate: 0.1               # Attention dropout
  drop_path_rate: 0.1               # Stochastic depth rate
  ape: false                        # Absolute position embedding
  patch_norm: true                  # Normalize after patch embedding
  use_checkpoint: false             # Gradient checkpointing (saves memory)
  final_upsample: "expand_first"    # Final upsampling strategy
  final_activation: "sigmoid"

training:
  epochs: 35
  
  optimizer:
    type: "adam"
    learning_rate: 0.0001
    weight_decay: 0.0001
  
  scheduler:
    type: "cosine"
    min_lr: 0.000001
  
  loss:
    type: "focal_tversky"
    alpha: 0.3
    beta: 0.7
    gamma: 1.333
    smooth: 0.000001
  
  metrics:
    - "dice"
    - "focal_tversky"
    - "auc"
  
  mixed_precision: false
  gradient_clip: 1.0
  
  validation:
    frequency: 1
  
  early_stopping:
    enabled: true
    patience: 15
    monitor: "val_focal_tversky"
    mode: "max"
  
  checkpoint:
    save_best: true
    save_last: true
    monitor: "val_focal_tversky"
    mode: "max"

logging:
  tensorboard: true
  log_images: true
  image_log_frequency: 1
  
  log_activations: true
  activation_log_frequency: 1
  activation_layers: "auto"

debug:
  profile_memory: false
  detailed_memory: true
  estimate_activations: true
  profile_training_step: false

seed: 42
device: "cuda"

