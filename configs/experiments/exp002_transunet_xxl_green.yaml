name: "exp002_transunet_xl_green"
description: "TransUNet XL training - Extra Large transformer with green channel"
tags: ["transunet", "transformer", "hybrid", "green", "xl"]

dataset: "configs/datasets/fives512_g.yaml"

output:
  dir: "outputs/experiments/exp002_transunet_xxl_green"
  save_predictions: true

data:
  batch_size: 1
  num_workers: 2
  pin_memory: true
  
  augmentation:
    enabled: false
  
  preprocessing:
    normalize: true
    pad_to_multiple: 32

model:
  type: "TransUNet"
  in_channels: 1
  out_channels: 1
  depths: [32, 64, 128, 256, 512]     # Same as UNet: [enc1, enc2, enc3, enc4, bottleneck]
  
  # Transformer configuration (increased for better capacity)
  transformer_depth: 10                # Number of transformer blocks (increased from 6)
  transformer_heads: 32               # Number of attention heads (must divide 512: 512/16=32)
  transformer_mlp_ratio: 8          # MLP expansion ratio (increased from 4.0)
  transformer_dropout: 0.1            # Dropout probability
  
  final_activation: "sigmoid"

training:
  epochs: 30
  
  optimizer:
    type: "adam"
    learning_rate: 0.0001
    weight_decay: 0.0001
  
  scheduler:
    type: "cosine"
    min_lr: 0.000001
  
  loss:
    type: "dice"
    smooth: 0.000001
  
  metrics:
    - "dice"
    - "iou"
  
  mixed_precision: false
  gradient_clip: 1.0
  
  validation:
    frequency: 1
  
  early_stopping:
    enabled: true
    patience: 10
    monitor: "val_dice"
    mode: "max"
  
  checkpoint:
    save_best: true
    save_last: true
    monitor: "val_dice"
    mode: "max"

logging:
  tensorboard: true
  log_images: true
  image_log_frequency: 1              # Log images every N epochs (if log_images: true)
  
  # Advanced logging options
  log_activations: false              # Enable to monitor layer activations
  activation_log_frequency: 5         # Log activations every N epochs (if log_activations: true)
  activation_layers: "auto"           # "auto" for model defaults, list for custom, null for all

# Debug options for memory profiling
debug:
  profile_memory: false               # Enable memory profiling before training
  detailed_memory: true               # Show per-layer breakdown
  estimate_activations: true          # Estimate activation memory (runs forward pass)
  profile_training_step: false        # Profile an actual training step (CUDA only)

seed: 42
device: "cuda" 

