# Quick Start Guide

## âœ… What's Been Implemented

A **minimal, working UNet pipeline** for vessel segmentation with:

- âœ“ Clean configuration system (Dataset + Experiment YAMLs)
- âœ“ Dataset loader with automatic normalization
- âœ“ Basic UNet architecture (5 levels: 32â†’64â†’128â†’256â†’512)
- âœ“ Dice loss function
- âœ“ Dice coefficient & IoU metrics
- âœ“ Training loop with validation
- âœ“ Early stopping & checkpointing
- âœ“ Testing script with predictions
- âœ“ Shell launcher for easy execution

## ğŸ“ Setup Your Data

Place your FIVES512 dataset here:
```
/home/vlv/Documents/master/deepLearning/project/codebase/data/FIVES512/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ image/ (*.png)
â”‚   â””â”€â”€ label/ (*.png)
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ image/
â”‚   â””â”€â”€ label/
â””â”€â”€ test/
    â”œâ”€â”€ image/
    â””â”€â”€ label/
```

Or update the path in `configs/datasets/fives_512.yaml`

## ğŸš€ Train Your First Model

```bash
# Option 1: Using the shell script
./train.sh exp001_basic_unet

# Option 2: Direct Python
python scripts/train.py --config configs/experiments/exp001_basic_unet.yaml
```

## ğŸ§ª Test the Model

```bash
python scripts/test.py --config outputs/experiments/exp001_basic_unet/config.yaml
```

This will:
- Load the best checkpoint
- Run inference on test set
- Calculate metrics (Dice, IoU)
- Save predictions to `outputs/experiments/exp001_basic_unet/predictions/`
- Save metrics to `outputs/experiments/exp001_basic_unet/metrics.json`

## ğŸ“Š Output Structure

After training, you'll find:
```
outputs/experiments/exp001_basic_unet/
â”œâ”€â”€ config.yaml              # Copy of experiment config
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best.pth            # Best validation model
â”‚   â””â”€â”€ last.pth            # Latest checkpoint
â”œâ”€â”€ predictions/            # Test predictions (after running test.py)
â””â”€â”€ metrics.json           # Final test metrics
```

## âš™ï¸ Modify the Configuration

Edit `configs/experiments/exp001_basic_unet.yaml`:

**Quick tweaks:**
- `training.epochs: 10` â†’ Change number of epochs
- `data.batch_size: 4` â†’ Adjust batch size for your GPU
- `model.depths: [32, 64, 128, 256, 512]` â†’ Make model smaller/larger
- `training.learning_rate: 0.0001` â†’ Tune learning rate

**Create a new experiment:**
```bash
cp configs/experiments/exp001_basic_unet.yaml configs/experiments/exp002_my_test.yaml
# Edit exp002_my_test.yaml
./train.sh exp002_my_test
```

## ğŸ”§ Current Limitations (By Design)

This is the **minimal working version**. Not yet implemented:
- âŒ Data augmentation (structure ready, not active)
- âŒ Attention mechanisms (skeleton in blocks/)
- âŒ Transformer blocks (skeleton in blocks/)
- âŒ Mixed precision training (flag exists but not active)
- âŒ TensorBoard logging (flag exists but not implemented)
- âŒ Additional loss functions (only Dice)

These will be added later following the pipeline structure!

## ğŸ› Troubleshooting

**"No images found in..."**
- Check your data path in `configs/datasets/fives_512.yaml`
- Ensure images are `.png` format

**CUDA out of memory**
- Reduce `batch_size` in experiment config
- Reduce model size: `depths: [16, 32, 64, 128, 256]`

**Import errors**
- Run from the codebase root: `cd /home/vlv/Documents/master/deepLearning/project/codebase`
- Check all `__init__.py` files exist

## ğŸ“ Next Steps

To extend the pipeline:

1. **Add data augmentation** â†’ Edit `src/data/transforms.py`
2. **Add new loss functions** â†’ Add to `src/training/losses.py`
3. **Add attention/transformers** â†’ Use skeletons in `src/models/blocks/`
4. **Add TensorBoard** â†’ Create logger in `src/utils/logger.py`

All structures are already in place per the specification! 